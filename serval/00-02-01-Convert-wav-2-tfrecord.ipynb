{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import ntpath\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "from scipy.io import wavfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./vggish')\n",
    "\n",
    "import vggish_input\n",
    "import vggish_postprocess\n",
    "import vggish_params\n",
    "import vggish_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project directory\n",
    "project_name     = 'amsterdam_custom_samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your serval data folder (should be correctly set already)\n",
    "serval_data_folder = \"../data\"\n",
    "project_data_folder = serval_data_folder + '/' + project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input files\n",
    "input_wav_sample_filepath = project_data_folder + '/csv_files/output_resampled_wav_samples.csv'\n",
    "target_tfrecord_folder    = project_data_folder + '/tfrecords_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## De vggish model folder\n",
    "pca_params_vggish_model_folder = serval_data_folder + \"/vggish_model/models\"\n",
    "print('Files in vggish model directory: (Expecting vggish_..., youtube_...)')\n",
    "print(os.listdir(pca_params_vggish_model_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete flag if it exists\n",
    "def del_attr(FLAGS, key):\n",
    "    if key in FLAGS._flags():\n",
    "        FLAGS.__delattr__(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "del_attr(FLAGS, 'f')\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "del_attr(FLAGS, 'audio_embedding_feature_name')\n",
    "flags.DEFINE_string(\n",
    "    'audio_embedding_feature_name', 'audio_embedding',\n",
    "    'Path to the VGGish checkpoint file.')\n",
    "\n",
    "del_attr(FLAGS, 'pca_params')\n",
    "flags.DEFINE_string(\n",
    "    'pca_params', pca_params_vggish_model_folder + '/vggish_pca_params.npz',\n",
    "    'Path to the VGGish PCA parameters file.')\n",
    "\n",
    "del_attr(FLAGS, 'checkpoint')\n",
    "flags.DEFINE_string(\n",
    "    'checkpoint', pca_params_vggish_model_folder + '/vggish_model.ckpt',\n",
    "    'Path to the VGGish checkpoint file.')\n",
    "\n",
    "# del_attr(FLAGS, 'yt_checkpoint')\n",
    "# flags.DEFINE_string(\n",
    "#    'yt_checkpoint', 'models/youtube_model.ckpt',\n",
    "#    'Path to the VGGish checkpoint file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read class labels\n",
    "df_wav_samples = pd.read_csv(input_wav_sample_filepath, sep=\";\")\n",
    "display(df_wav_samples.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Function that takes examples from wav-file as input and returns a sequence example\n",
    "\n",
    "## Function is depricated\n",
    "\n",
    "if False:\n",
    "    def getSequenceExample(examples_batch, labels, video_id=[b'-1LrH01Ei1w']):\n",
    "        with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "            # Prepare a postprocessor to munge the model embeddings.\n",
    "            pproc = vggish_postprocess.Postprocessor(FLAGS.pca_params)\n",
    "\n",
    "            # Define the model: load the checkpoint and locate input and output tensors\n",
    "            # Input: [batch_size, num_frames, num_bands] \n",
    "            # where [num_frames, num_bands] represents log-mel-scale spectrogram\n",
    "            # Output: embeddings\n",
    "            vggish_slim.define_vggish_slim(training=False)\n",
    "            vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n",
    "\n",
    "            features_tensor = sess.graph.get_tensor_by_name(\n",
    "                vggish_params.VGGISH_INPUT_TENSOR_NAME)\n",
    "            embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "                vggish_params.VGGISH_OUTPUT_TENSOR_NAME)\n",
    "\n",
    "            # Run inference and postprocessing.\n",
    "            [embedding_batch] = sess.run([embedding_tensor],\n",
    "                                         feed_dict={features_tensor: examples_batch})\n",
    "\n",
    "            print(embedding_batch.shape)\n",
    "            if(embedding_batch.shape!=10):\n",
    "                print(\"*****************************************************************\")\n",
    "\n",
    "            postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "            #print(postprocessed_batch)\n",
    "\n",
    "\n",
    "            ## Maak labels en video-id voor in de example\n",
    "            label_feat = tf.train.Feature(int64_list=tf.train.Int64List(value=labels))\n",
    "            videoid_feat = tf.train.Feature(bytes_list=tf.train.BytesList(value=video_id))\n",
    "\n",
    "            ## Maak sequence example\n",
    "            seq_example = tf.train.SequenceExample(\n",
    "                context = tf.train.Features(feature={\"labels\": label_feat, \"video_id\": videoid_feat}),\n",
    "                feature_lists = tf.train.FeatureLists(\n",
    "                    feature_list={\n",
    "                        FLAGS.audio_embedding_feature_name:\n",
    "                            tf.train.FeatureList(\n",
    "                                feature=[\n",
    "                                    tf.train.Feature(\n",
    "                                        bytes_list=tf.train.BytesList(\n",
    "                                            value=[embedding.tobytes()]))\n",
    "                                    for embedding in postprocessed_batch\n",
    "                                ]\n",
    "                            )\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return(seq_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is depricated\n",
    "if False:\n",
    "    # Select files\n",
    "    files = df_wav_samples.filepath.unique()\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        ## Fetch labels\n",
    "        df_labels = df_wav_samples.loc[df_wav_samples.filepath == file].copy(deep=True)\n",
    "\n",
    "        if len(df_labels) == 0:\n",
    "            print('[ERROR]: Found sample without labels; filename=', file, sep='')\n",
    "            continue\n",
    "\n",
    "        ## Get ind_train\n",
    "        ind_train = df_labels.ind_train.unique()\n",
    "        if len(ind_train) != 1 or ind_train[0] not in ['train', 'eval']:\n",
    "            print('[ERROR]: Found label that is not train or eval; filename=', file, '; ind_train=', df_labels.ind_train, sep='')\n",
    "            continue\n",
    "\n",
    "        ## .. encode - Part 1\n",
    "        examples_batch = vggish_input.wavfile_to_examples(file)\n",
    "\n",
    "        if(examples_batch.shape[0]>10):\n",
    "            nr_sec = examples_batch.shape[0]\n",
    "            print(nr_sec)\n",
    "            start = int(math.floor((nr_sec-10)/2))\n",
    "            print(\"start: \" + str(start))\n",
    "            end = int(nr_sec-math.ceil((nr_sec-10)/2))\n",
    "            print(\"end: \" + str(end))\n",
    "            examples_batch = examples_batch[start:end, :, :]\n",
    "            print(examples_batch.shape)\n",
    "\n",
    "        ## .. encode - Part 2\n",
    "        seq_example = getSequenceExample(examples_batch, df_labels.label.unique())\n",
    "\n",
    "        ## .. write example - Part 3\n",
    "        if ind_train == 'train':\n",
    "            tf_record_writer_train.write(seq_example.SerializeToString())\n",
    "        else:\n",
    "            tf_record_writer_eval.write(seq_example.SerializeToString())\n",
    "\n",
    "    tf_record_writer_train.close()\n",
    "    tf_record_writer_eval.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_write_single_wav(file, df_wav_samples, sess, pproc, tf_record_writer_train, tf_record_writer_eval, video_id=[b'-1LrH01Ei1w']):\n",
    "    ## Fetch labels\n",
    "    df_labels = df_wav_samples.loc[df_wav_samples.filepath == file].copy(deep=True)\n",
    "    \n",
    "    if len(df_labels) == 0:\n",
    "        print('[ERROR]: Found sample without labels; filename=', file, sep='')\n",
    "        return\n",
    "\n",
    "    ## Get ind_train\n",
    "    ind_train = df_labels.ind_train.unique()\n",
    "    if len(ind_train) != 1 or ind_train[0] not in ['train', 'eval']:\n",
    "        print('[ERROR]: Found label that is not train or eval; filename=', file, '; ind_train=', df_labels.ind_train, sep='')\n",
    "        return\n",
    "\n",
    "    ## .. encode - Part 1\n",
    "    examples_batch = vggish_input.wavfile_to_examples(file)\n",
    "\n",
    "    if(examples_batch.shape[0]>10):\n",
    "        print(str(file))\n",
    "        nr_sec = examples_batch.shape[0]\n",
    "        print(nr_sec)\n",
    "        start = int(math.floor((nr_sec-10)/2))\n",
    "        print(\"start: \" + str(start))\n",
    "        end = int(nr_sec-math.ceil((nr_sec-10)/2))\n",
    "        print(\"end: \" + str(end))\n",
    "        examples_batch = examples_batch[start:end, :, :]\n",
    "        print(examples_batch.shape)\n",
    "        \n",
    "    if(examples_batch.shape[0]<10):\n",
    "        return(False)\n",
    "\n",
    "    # .. Run inference and postprocessing - Part 2\n",
    "    features_tensor = sess.graph.get_tensor_by_name(vggish_params.VGGISH_INPUT_TENSOR_NAME)\n",
    "    embedding_tensor = sess.graph.get_tensor_by_name(vggish_params.VGGISH_OUTPUT_TENSOR_NAME)\n",
    "\n",
    "    [embedding_batch] = sess.run([embedding_tensor],\n",
    "                                 feed_dict={features_tensor: examples_batch})\n",
    "\n",
    "    if(embedding_batch.shape!=(10, 128)):\n",
    "        print(embedding_batch.shape)\n",
    "        print(str(file))\n",
    "        print(\"*****************************************************************\")\n",
    "\n",
    "    # .. Get result - Part 3\n",
    "    postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "\n",
    "    ## .. Maak labels en video-id voor in de example - Part 4\n",
    "    label_feat = tf.train.Feature(int64_list=tf.train.Int64List(value=df_labels.label.unique()))\n",
    "    videoid_feat = tf.train.Feature(bytes_list=tf.train.BytesList(value=video_id))\n",
    "\n",
    "    ## .. Maak sequence example - Part 5\n",
    "    seq_example = tf.train.SequenceExample(\n",
    "        context = tf.train.Features(feature={\"labels\": label_feat, \"video_id\": videoid_feat}),\n",
    "        feature_lists = tf.train.FeatureLists(\n",
    "            feature_list={\n",
    "                FLAGS.audio_embedding_feature_name:\n",
    "                    tf.train.FeatureList(\n",
    "                        feature=[\n",
    "                            tf.train.Feature(\n",
    "                                bytes_list=tf.train.BytesList(\n",
    "                                    value=[embedding.tobytes()]))\n",
    "                            for embedding in postprocessed_batch\n",
    "                        ]\n",
    "                    )\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ## .. write example - Part 3\n",
    "    if ind_train == 'train':\n",
    "        tf_record_writer_train.write(seq_example.SerializeToString())\n",
    "    else:\n",
    "        tf_record_writer_eval.write(seq_example.SerializeToString())\n",
    "        \n",
    "    return(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that takes examples from wav-file as input and returns a sequence example\n",
    "\n",
    "def convert_wav_samples(files, df_wav_samples):\n",
    "    # setup a writer\n",
    "    ## timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    timestr = time.strftime(\"%Y%m%d\")\n",
    "    tf_record_writer_train = tf.python_io.TFRecordWriter(target_tfrecord_folder + '/train_' + timestr + '.tfrecord')\n",
    "    tf_record_writer_eval = tf.python_io.TFRecordWriter(target_tfrecord_folder + '/eval_' + timestr + '.tfrecord')\n",
    "\n",
    "    corrupt_files = []\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "        # Prepare a postprocessor to munge the model embeddings.\n",
    "        pproc = vggish_postprocess.Postprocessor(FLAGS.pca_params)\n",
    "    \n",
    "        # Define the model: load the checkpoint and locate input and output tensors\n",
    "        # Input: [batch_size, num_frames, num_bands] \n",
    "        # where [num_frames, num_bands] represents log-mel-scale spectrogram\n",
    "        # Output: embeddings\n",
    "        vggish_slim.define_vggish_slim(training=False)\n",
    "        vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n",
    "\n",
    "        ## .. load and encode file - Part 1\n",
    "        for file in tqdm(files):\n",
    "            result = convert_and_write_single_wav(file, df_wav_samples, sess, pproc, tf_record_writer_train, tf_record_writer_eval)\n",
    "            if not result:\n",
    "                corrupt_files.append(file)\n",
    "                \n",
    "    tf_record_writer_train.close()\n",
    "    tf_record_writer_eval.close()\n",
    "    return(corrupt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = df_wav_samples.filepath.unique()\n",
    "skipped_files = convert_wav_samples(files, df_wav_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(set(skipped_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
