{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import ntpath\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "from scipy.io import wavfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./vggish')\n",
    "\n",
    "import vggish_input\n",
    "import vggish_postprocess\n",
    "import vggish_params\n",
    "import vggish_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project directory\n",
    "project_name     = 'amsterdam_custom_samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your serval data folder (should be correctly set already)\n",
    "serval_data_folder = \"../data\"\n",
    "project_data_folder = serval_data_folder + '/' + project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input files\n",
    "input_wav_sample_filepath = project_data_folder + '/csv_files/03_output_resampled_wav_samples.csv'\n",
    "target_tfrecord_folder    = project_data_folder + '/tfrecords_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in vggish model directory: (Expecting vggish_..., youtube_...)\n",
      "['vggish_model.ckpt', 'vggish_pca_params.npz']\n"
     ]
    }
   ],
   "source": [
    "## De vggish model folder\n",
    "pca_params_vggish_model_folder = serval_data_folder + \"/vggish_model/models\"\n",
    "print('Files in vggish model directory: (Expecting vggish_..., youtube_...)')\n",
    "print(os.listdir(pca_params_vggish_model_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete flag if it exists\n",
    "def del_attr(FLAGS, key):\n",
    "    if key in FLAGS._flags():\n",
    "        FLAGS.__delattr__(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "del_attr(FLAGS, 'f')\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "del_attr(FLAGS, 'audio_embedding_feature_name')\n",
    "flags.DEFINE_string(\n",
    "    'audio_embedding_feature_name', 'audio_embedding',\n",
    "    'Path to the VGGish checkpoint file.')\n",
    "\n",
    "del_attr(FLAGS, 'pca_params')\n",
    "flags.DEFINE_string(\n",
    "    'pca_params', pca_params_vggish_model_folder + '/vggish_pca_params.npz',\n",
    "    'Path to the VGGish PCA parameters file.')\n",
    "\n",
    "del_attr(FLAGS, 'checkpoint')\n",
    "flags.DEFINE_string(\n",
    "    'checkpoint', pca_params_vggish_model_folder + '/vggish_model.ckpt',\n",
    "    'Path to the VGGish checkpoint file.')\n",
    "\n",
    "# del_attr(FLAGS, 'yt_checkpoint')\n",
    "# flags.DEFINE_string(\n",
    "#    'yt_checkpoint', 'models/youtube_model.ckpt',\n",
    "#    'Path to the VGGish checkpoint file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>mid</th>\n",
       "      <th>display_name</th>\n",
       "      <th>source</th>\n",
       "      <th>filename</th>\n",
       "      <th>filepath</th>\n",
       "      <th>old_filename</th>\n",
       "      <th>old_filepath</th>\n",
       "      <th>ind_train</th>\n",
       "      <th>display_name.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>/c/a_1001</td>\n",
       "      <td>556 gunshots</td>\n",
       "      <td>custom_amsterdam_sample</td>\n",
       "      <td>../data/amsterdam_custom_samples/wav_samples/e...</td>\n",
       "      <td>../data/amsterdam_custom_samples/wav_samples/e...</td>\n",
       "      <td>shot556DL2.319.ch01.180718.180832.48..wav</td>\n",
       "      <td>../data/wav_samples_custom/556_gunshots/shot55...</td>\n",
       "      <td>eval</td>\n",
       "      <td>556 gunshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>/c/a_1001</td>\n",
       "      <td>556 gunshots</td>\n",
       "      <td>custom_amsterdam_sample</td>\n",
       "      <td>../data/amsterdam_custom_samples/wav_samples/e...</td>\n",
       "      <td>../data/amsterdam_custom_samples/wav_samples/e...</td>\n",
       "      <td>shot556DL2.319.ch01.180718.180832.48..wav</td>\n",
       "      <td>../data/wav_samples_custom/556_gunshots/shot55...</td>\n",
       "      <td>eval</td>\n",
       "      <td>556 gunshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>/c/a_1001</td>\n",
       "      <td>556 gunshots</td>\n",
       "      <td>custom_amsterdam_sample</td>\n",
       "      <td>../data/amsterdam_custom_samples/wav_samples/e...</td>\n",
       "      <td>../data/amsterdam_custom_samples/wav_samples/e...</td>\n",
       "      <td>shot556DL2.319.ch01.180718.180832.48..wav</td>\n",
       "      <td>../data/wav_samples_custom/556_gunshots/shot55...</td>\n",
       "      <td>eval</td>\n",
       "      <td>556 gunshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>/c/a_1001</td>\n",
       "      <td>556 gunshots</td>\n",
       "      <td>custom_amsterdam_sample</td>\n",
       "      <td>../data/amsterdam_custom_samples/wav_samples/e...</td>\n",
       "      <td>../data/amsterdam_custom_samples/wav_samples/e...</td>\n",
       "      <td>shot556DL2.140.ch01.180718.180218.22..wav</td>\n",
       "      <td>../data/wav_samples_custom/556_gunshots/shot55...</td>\n",
       "      <td>eval</td>\n",
       "      <td>556 gunshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>/c/a_1001</td>\n",
       "      <td>556 gunshots</td>\n",
       "      <td>custom_amsterdam_sample</td>\n",
       "      <td>../data/amsterdam_custom_samples/wav_samples/e...</td>\n",
       "      <td>../data/amsterdam_custom_samples/wav_samples/e...</td>\n",
       "      <td>shot556DL2.140.ch01.180718.180218.22..wav</td>\n",
       "      <td>../data/wav_samples_custom/556_gunshots/shot55...</td>\n",
       "      <td>eval</td>\n",
       "      <td>556 gunshots</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label        mid  display_name                   source  \\\n",
       "0   1001  /c/a_1001  556 gunshots  custom_amsterdam_sample   \n",
       "1   1001  /c/a_1001  556 gunshots  custom_amsterdam_sample   \n",
       "2   1001  /c/a_1001  556 gunshots  custom_amsterdam_sample   \n",
       "3   1001  /c/a_1001  556 gunshots  custom_amsterdam_sample   \n",
       "4   1001  /c/a_1001  556 gunshots  custom_amsterdam_sample   \n",
       "\n",
       "                                            filename  \\\n",
       "0  ../data/amsterdam_custom_samples/wav_samples/e...   \n",
       "1  ../data/amsterdam_custom_samples/wav_samples/e...   \n",
       "2  ../data/amsterdam_custom_samples/wav_samples/e...   \n",
       "3  ../data/amsterdam_custom_samples/wav_samples/e...   \n",
       "4  ../data/amsterdam_custom_samples/wav_samples/e...   \n",
       "\n",
       "                                            filepath  \\\n",
       "0  ../data/amsterdam_custom_samples/wav_samples/e...   \n",
       "1  ../data/amsterdam_custom_samples/wav_samples/e...   \n",
       "2  ../data/amsterdam_custom_samples/wav_samples/e...   \n",
       "3  ../data/amsterdam_custom_samples/wav_samples/e...   \n",
       "4  ../data/amsterdam_custom_samples/wav_samples/e...   \n",
       "\n",
       "                                old_filename  \\\n",
       "0  shot556DL2.319.ch01.180718.180832.48..wav   \n",
       "1  shot556DL2.319.ch01.180718.180832.48..wav   \n",
       "2  shot556DL2.319.ch01.180718.180832.48..wav   \n",
       "3  shot556DL2.140.ch01.180718.180218.22..wav   \n",
       "4  shot556DL2.140.ch01.180718.180218.22..wav   \n",
       "\n",
       "                                        old_filepath ind_train display_name.1  \n",
       "0  ../data/wav_samples_custom/556_gunshots/shot55...      eval   556 gunshots  \n",
       "1  ../data/wav_samples_custom/556_gunshots/shot55...      eval   556 gunshots  \n",
       "2  ../data/wav_samples_custom/556_gunshots/shot55...      eval   556 gunshots  \n",
       "3  ../data/wav_samples_custom/556_gunshots/shot55...      eval   556 gunshots  \n",
       "4  ../data/wav_samples_custom/556_gunshots/shot55...      eval   556 gunshots  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read class labels\n",
    "df_wav_samples = pd.read_csv(input_wav_sample_filepath, sep=\";\")\n",
    "display(df_wav_samples.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1001, 1003, 1002, 1004, 1005, 1006, 1007, 1000, 1008, 1009])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wav_samples.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Function that takes examples from wav-file as input and returns a sequence example\n",
    "\n",
    "## Function is depriciated\n",
    "\n",
    "if False:\n",
    "    def getSequenceExample(examples_batch, labels, video_id=[b'-1LrH01Ei1w']):\n",
    "        with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "            # Prepare a postprocessor to munge the model embeddings.\n",
    "            pproc = vggish_postprocess.Postprocessor(FLAGS.pca_params)\n",
    "\n",
    "            # Define the model: load the checkpoint and locate input and output tensors\n",
    "            # Input: [batch_size, num_frames, num_bands] \n",
    "            # where [num_frames, num_bands] represents log-mel-scale spectrogram\n",
    "            # Output: embeddings\n",
    "            vggish_slim.define_vggish_slim(training=False)\n",
    "            vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n",
    "\n",
    "            features_tensor = sess.graph.get_tensor_by_name(\n",
    "                vggish_params.VGGISH_INPUT_TENSOR_NAME)\n",
    "            embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "                vggish_params.VGGISH_OUTPUT_TENSOR_NAME)\n",
    "\n",
    "            # Run inference and postprocessing.\n",
    "            [embedding_batch] = sess.run([embedding_tensor],\n",
    "                                         feed_dict={features_tensor: examples_batch})\n",
    "\n",
    "            print(embedding_batch.shape)\n",
    "            if(embedding_batch.shape!=10):\n",
    "                print(\"*****************************************************************\")\n",
    "\n",
    "            postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "            #print(postprocessed_batch)\n",
    "\n",
    "\n",
    "            ## Maak labels en video-id voor in de example\n",
    "            label_feat = tf.train.Feature(int64_list=tf.train.Int64List(value=labels))\n",
    "            videoid_feat = tf.train.Feature(bytes_list=tf.train.BytesList(value=video_id))\n",
    "\n",
    "            ## Maak sequence example\n",
    "            seq_example = tf.train.SequenceExample(\n",
    "                context = tf.train.Features(feature={\"labels\": label_feat, \"video_id\": videoid_feat}),\n",
    "                feature_lists = tf.train.FeatureLists(\n",
    "                    feature_list={\n",
    "                        FLAGS.audio_embedding_feature_name:\n",
    "                            tf.train.FeatureList(\n",
    "                                feature=[\n",
    "                                    tf.train.Feature(\n",
    "                                        bytes_list=tf.train.BytesList(\n",
    "                                            value=[embedding.tobytes()]))\n",
    "                                    for embedding in postprocessed_batch\n",
    "                                ]\n",
    "                            )\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return(seq_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is depricated\n",
    "if False:\n",
    "    # Select files\n",
    "    files = df_wav_samples.filepath.unique()\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        ## Fetch labels\n",
    "        df_labels = df_wav_samples.loc[df_wav_samples.filepath == file].copy(deep=True)\n",
    "\n",
    "        if len(df_labels) == 0:\n",
    "            print('[ERROR]: Found sample without labels; filename=', file, sep='')\n",
    "            continue\n",
    "\n",
    "        ## Get ind_train\n",
    "        ind_train = df_labels.ind_train.unique()\n",
    "        if len(ind_train) != 1 or ind_train[0] not in ['train', 'eval']:\n",
    "            print('[ERROR]: Found label that is not train or eval; filename=', file, '; ind_train=', df_labels.ind_train, sep='')\n",
    "            continue\n",
    "\n",
    "        ## .. encode - Part 1\n",
    "        examples_batch = vggish_input.wavfile_to_examples(file)\n",
    "\n",
    "        if(examples_batch.shape[0]>10):\n",
    "            nr_sec = examples_batch.shape[0]\n",
    "            print(nr_sec)\n",
    "            start = int(math.floor((nr_sec-10)/2))\n",
    "            print(\"start: \" + str(start))\n",
    "            end = int(nr_sec-math.ceil((nr_sec-10)/2))\n",
    "            print(\"end: \" + str(end))\n",
    "            examples_batch = examples_batch[start:end, :, :]\n",
    "            print(examples_batch.shape)\n",
    "\n",
    "        ## .. encode - Part 2\n",
    "        seq_example = getSequenceExample(examples_batch, df_labels.label.unique())\n",
    "\n",
    "        ## .. write example - Part 3\n",
    "        if ind_train == 'train':\n",
    "            tf_record_writer_train.write(seq_example.SerializeToString())\n",
    "        else:\n",
    "            tf_record_writer_eval.write(seq_example.SerializeToString())\n",
    "\n",
    "    tf_record_writer_train.close()\n",
    "    tf_record_writer_eval.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_write_single_wav(file, df_wav_samples, sess, pproc, tf_record_writer_train, tf_record_writer_eval, video_id=[b'-1LrH01Ei1w']):\n",
    "    ## Fetch labels\n",
    "    df_labels = df_wav_samples.loc[df_wav_samples.filepath == file].copy(deep=True)\n",
    "    \n",
    "    if len(df_labels) == 0:\n",
    "        print('[ERROR]: Found sample without labels; filename=', file, sep='')\n",
    "        return\n",
    "\n",
    "    ## Get ind_train\n",
    "    ind_train = df_labels.ind_train.unique()\n",
    "    if len(ind_train) != 1 or ind_train[0] not in ['train', 'eval']:\n",
    "        print('[ERROR]: Found label that is not train or eval; filename=', file, '; ind_train=', df_labels.ind_train, sep='')\n",
    "        return\n",
    "\n",
    "    ## .. encode - Part 1\n",
    "    examples_batch = vggish_input.wavfile_to_examples(file)\n",
    "\n",
    "    if(examples_batch.shape[0]>10):\n",
    "        print(str(file))\n",
    "        nr_sec = examples_batch.shape[0]\n",
    "        print(nr_sec)\n",
    "        start = int(math.floor((nr_sec-10)/2))\n",
    "        print(\"start: \" + str(start))\n",
    "        end = int(nr_sec-math.ceil((nr_sec-10)/2))\n",
    "        print(\"end: \" + str(end))\n",
    "        examples_batch = examples_batch[start:end, :, :]\n",
    "        print(examples_batch.shape)\n",
    "        \n",
    "    if(examples_batch.shape[0]<10):\n",
    "        return(False)\n",
    "\n",
    "    # .. Run inference and postprocessing - Part 2\n",
    "    features_tensor = sess.graph.get_tensor_by_name(vggish_params.VGGISH_INPUT_TENSOR_NAME)\n",
    "    embedding_tensor = sess.graph.get_tensor_by_name(vggish_params.VGGISH_OUTPUT_TENSOR_NAME)\n",
    "\n",
    "    [embedding_batch] = sess.run([embedding_tensor],\n",
    "                                 feed_dict={features_tensor: examples_batch})\n",
    "\n",
    "    if(embedding_batch.shape!=(10, 128)):\n",
    "        print(embedding_batch.shape)\n",
    "        print(str(file))\n",
    "        print(\"*****************************************************************\")\n",
    "\n",
    "    # .. Get result - Part 3\n",
    "    postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "\n",
    "    ## .. Maak labels en video-id voor in de example - Part 4\n",
    "    label_feat = tf.train.Feature(int64_list=tf.train.Int64List(value=df_labels.label.unique()))\n",
    "    videoid_feat = tf.train.Feature(bytes_list=tf.train.BytesList(value=video_id))\n",
    "\n",
    "    ## .. Maak sequence example - Part 5\n",
    "    seq_example = tf.train.SequenceExample(\n",
    "        context = tf.train.Features(feature={\"labels\": label_feat, \"video_id\": videoid_feat}),\n",
    "        feature_lists = tf.train.FeatureLists(\n",
    "            feature_list={\n",
    "                FLAGS.audio_embedding_feature_name:\n",
    "                    tf.train.FeatureList(\n",
    "                        feature=[\n",
    "                            tf.train.Feature(\n",
    "                                bytes_list=tf.train.BytesList(\n",
    "                                    value=[embedding.tobytes()]))\n",
    "                            for embedding in postprocessed_batch\n",
    "                        ]\n",
    "                    )\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ## .. write example - Part 3\n",
    "    if ind_train == 'train':\n",
    "        tf_record_writer_train.write(seq_example.SerializeToString())\n",
    "    else:\n",
    "        tf_record_writer_eval.write(seq_example.SerializeToString())\n",
    "        \n",
    "    return(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that takes examples from wav-file as input and returns a sequence example\n",
    "\n",
    "def convert_wav_samples(files, df_wav_samples):\n",
    "    # setup a writer\n",
    "    ## timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    timestr = time.strftime(\"%Y%m%d\")\n",
    "    tf_record_writer_train = tf.python_io.TFRecordWriter(target_tfrecord_folder + '/train_' + timestr + '.tfrecord')\n",
    "    tf_record_writer_eval = tf.python_io.TFRecordWriter(target_tfrecord_folder + '/eval_' + timestr + '.tfrecord')\n",
    "\n",
    "    corrupt_files = []\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "        # Prepare a postprocessor to munge the model embeddings.\n",
    "        pproc = vggish_postprocess.Postprocessor(FLAGS.pca_params)\n",
    "    \n",
    "        # Define the model: load the checkpoint and locate input and output tensors\n",
    "        # Input: [batch_size, num_frames, num_bands] \n",
    "        # where [num_frames, num_bands] represents log-mel-scale spectrogram\n",
    "        # Output: embeddings\n",
    "        vggish_slim.define_vggish_slim(training=False)\n",
    "        vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n",
    "\n",
    "        ## .. load and encode file - Part 1\n",
    "        for file in tqdm(files):\n",
    "            # check if file is there\n",
    "            if os.path.exists(file):\n",
    "                result = convert_and_write_single_wav(file, df_wav_samples, sess, pproc, tf_record_writer_train, tf_record_writer_eval)\n",
    "                if not result:\n",
    "                    corrupt_files.append(file)\n",
    "            else :\n",
    "                corrupt_files.append(file)\n",
    "                print('File not found : '+ str(file))\n",
    "                \n",
    "                \n",
    "    tf_record_writer_train.close()\n",
    "    tf_record_writer_eval.close()\n",
    "    return(corrupt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ./vggish/vggish_slim.py:79: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From ./vggish/vggish_slim.py:81: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From ./vggish/vggish_slim.py:123: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From ./vggish/vggish_slim.py:131: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ../data/vggish_model/models/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4099/58276 [02:19<26:04, 34.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/amsterdam_custom_samples/wav_samples/train/0db/b_brommers/0db_B.31.191231.150056.28.wav\n",
      "13\n",
      "start: 1\n",
      "end: 11\n",
      "(10, 96, 64)\n",
      "../data/amsterdam_custom_samples/wav_samples/train/6db/b_brommers/6db_B.31.191231.150056.28.wav\n",
      "13\n",
      "start: 1\n",
      "end: 11\n",
      "(10, 96, 64)\n",
      "../data/amsterdam_custom_samples/wav_samples/train/12db/b_brommers/12db_B.31.191231.150056.28.wav\n",
      "13\n",
      "start: 1\n",
      "end: 11\n",
      "(10, 96, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 5187/58276 [02:53<35:06, 25.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/amsterdam_custom_samples/wav_samples/train/0db/c_claxons/0db_C.36.191231.150634.22.wav\n",
      "30\n",
      "start: 10\n",
      "end: 20\n",
      "(10, 96, 64)\n",
      "../data/amsterdam_custom_samples/wav_samples/train/6db/c_claxons/6db_C.36.191231.150634.22.wav\n",
      "30\n",
      "start: 10\n",
      "end: 20\n",
      "(10, 96, 64)\n",
      "../data/amsterdam_custom_samples/wav_samples/train/12db/c_claxons/12db_C.36.191231.150634.22.wav\n",
      "30\n",
      "start: 10\n",
      "end: 20\n",
      "(10, 96, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58276/58276 [32:29<00:00, 29.90it/s]\n"
     ]
    }
   ],
   "source": [
    "files = df_wav_samples.filepath.unique()\n",
    "skipped_files = convert_wav_samples(files, df_wav_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(set(skipped_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
