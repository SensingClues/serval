{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "import utils\n",
    "import eval_util\n",
    "import losses\n",
    "import frame_level_models\n",
    "import video_level_models\n",
    "import readers\n",
    "import tensorflow as tf\n",
    "from tensorflow import app\n",
    "from tensorflow import flags\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset flags.\n",
    "flags.DEFINE_string(\"train_dir\", \"models\",\n",
    "                  \"The directory to load the model files from. \"\n",
    "                  \"The tensorboard metrics files are also saved to this \"\n",
    "                  \"directory.\")\n",
    "flags.DEFINE_string(\n",
    "  \"eval_data_pattern\", \"tfrecords/test_data_new/*.tfrecord\",\n",
    "  \"File glob defining the evaluation dataset in tensorflow.SequenceExample \"\n",
    "  \"format. The SequenceExamples are expected to have an 'rgb' byte array \"\n",
    "  \"sequence feature as well as a 'labels' int64 context feature.\")\n",
    "flags.DEFINE_string(\"feature_names\", \"audio_embedding\", \"Name of the feature \"\n",
    "                  \"to use for training.\")\n",
    "flags.DEFINE_string(\"feature_sizes\", \"128\", \"Length of the feature vectors.\")\n",
    "flags.DEFINE_integer(\"num_classes\", 37, \"Number of classes in dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model flags\n",
    "flags.DEFINE_bool(\n",
    "  \"frame_features\", True,\n",
    "  \"If set, then --eval_data_pattern must be frame-level features. \"\n",
    "  \"Otherwise, --eval_data_pattern must be aggregated video-level \"\n",
    "  \"features. The model must also be set appropriately (i.e. to read 3D \"\n",
    "  \"batches VS 4D batches.\")\n",
    "flags.DEFINE_string(\n",
    "  \"model\", \"FrameLevelLogisticModel\",\n",
    "  \"Which architecture to use for the model. Options include 'Logistic', \"\n",
    "  \"'SingleMixtureMoe', and 'TwoLayerSigmoid'. See aggregated_models.py and \"\n",
    "  \"frame_level_models.py for the model definitions.\")\n",
    "flags.DEFINE_integer(\"batch_size\", 512,\n",
    "                   \"How many examples to process per batch.\")\n",
    "flags.DEFINE_string(\"label_loss\", \"CrossEntropyLoss\",\n",
    "                  \"Loss computed on validation data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other flags.\n",
    "flags.DEFINE_integer(\"num_readers\", 8,\n",
    "                   \"How many threads to use for reading input files.\")\n",
    "flags.DEFINE_boolean(\"run_once\", True, \"Whether to run eval only once.\")\n",
    "flags.DEFINE_integer(\"top_k\", 10, \"How many predictions to output per video.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HK  get ground truth\n",
    "ground_thruth = eval_util.get_labels('csv_files/class_labels_indices_proportions.csv')\n",
    "gt_labels = [ground_thruth[k] for k in ground_thruth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(tf.logging.INFO)\n",
    "print(\"tensorflow version: %s\" % tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_class_by_name(name, modules):\n",
    "  \"\"\"Searches the provided modules for the named class and returns it.\"\"\"\n",
    "  modules = [getattr(module, name, None) for module in modules]\n",
    "  return next(a for a in modules if a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_evaluation_tensors(reader,\n",
    "                                 data_pattern,\n",
    "                                 batch_size=1024,\n",
    "                                 num_readers=1):\n",
    "  \"\"\"Creates the section of the graph which reads the evaluation data.\n",
    "\n",
    "  Args:\n",
    "    reader: A class which parses the training data.\n",
    "    data_pattern: A 'glob' style path to the data files.\n",
    "    batch_size: How many examples to process at a time.\n",
    "    num_readers: How many I/O threads to use.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing the features tensor, labels tensor, and optionally a\n",
    "    tensor containing the number of frames per video. The exact dimensions\n",
    "    depend on the reader being used.\n",
    "\n",
    "  Raises:\n",
    "    IOError: If no files matching the given pattern were found.\n",
    "  \"\"\"\n",
    "  logging.info(\"Using batch size of \" + str(batch_size) + \" for evaluation.\")\n",
    "  with tf.name_scope(\"eval_input\"):\n",
    "    files = gfile.Glob(data_pattern)\n",
    "    if not files:\n",
    "      raise IOError(\"Unable to find the evaluation files.\")\n",
    "    logging.info(\"number of evaluation files: \" + str(len(files)))\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        files, shuffle=False, num_epochs=1)\n",
    "    eval_data = [\n",
    "        reader.prepare_reader(filename_queue) for _ in range(num_readers)\n",
    "    ]\n",
    "    return tf.train.batch_join(\n",
    "        eval_data,\n",
    "        batch_size=batch_size,\n",
    "        capacity=3 * batch_size,\n",
    "        allow_smaller_final_batch=True,\n",
    "        enqueue_many=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(reader,\n",
    "                model,\n",
    "                eval_data_pattern,\n",
    "                label_loss_fn,\n",
    "                batch_size=1024,\n",
    "                num_readers=1):\n",
    "  \"\"\"Creates the Tensorflow graph for evaluation.\n",
    "\n",
    "  Args:\n",
    "    reader: The data file reader. It should inherit from BaseReader.\n",
    "    model: The core model (e.g. logistic or neural net). It should inherit\n",
    "           from BaseModel.\n",
    "    eval_data_pattern: glob path to the evaluation data files.\n",
    "    label_loss_fn: What kind of loss to apply to the model. It should inherit\n",
    "                from BaseLoss.\n",
    "    batch_size: How many examples to process at a time.\n",
    "    num_readers: How many threads to use for I/O operations.\n",
    "  \"\"\"\n",
    "\n",
    "  global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "  video_id_batch, model_input_raw, labels_batch, num_frames = get_input_evaluation_tensors(  # pylint: disable=g-line-too-long\n",
    "      reader,\n",
    "      eval_data_pattern,\n",
    "      batch_size=batch_size,\n",
    "      num_readers=num_readers)\n",
    "  tf.summary.histogram(\"model_input_raw\", model_input_raw)\n",
    "  \n",
    "  feature_dim = len(model_input_raw.get_shape()) - 1\n",
    "\n",
    "  # Normalize input features.\n",
    "  model_input = tf.nn.l2_normalize(model_input_raw, feature_dim)\n",
    "\n",
    "  with tf.variable_scope(\"tower\"):\n",
    "    result = model.create_model(model_input,\n",
    "                                num_frames=num_frames,\n",
    "                                vocab_size=reader.num_classes,\n",
    "                                labels=labels_batch,\n",
    "                                is_training=False)\n",
    "    predictions = result[\"predictions\"]\n",
    "    tf.summary.histogram(\"model_activations\", predictions)\n",
    "    if \"loss\" in result.keys():\n",
    "      label_loss = result[\"loss\"]\n",
    "    else:\n",
    "      label_loss = label_loss_fn.calculate_loss(predictions, labels_batch)\n",
    "\n",
    "  tf.add_to_collection(\"global_step\", global_step)\n",
    "  tf.add_to_collection(\"loss\", label_loss)\n",
    "  tf.add_to_collection(\"predictions\", predictions)\n",
    "  tf.add_to_collection(\"input_batch\", model_input)\n",
    "  tf.add_to_collection(\"video_id_batch\", video_id_batch)\n",
    "  tf.add_to_collection(\"num_frames\", num_frames)\n",
    "  tf.add_to_collection(\"labels\", tf.cast(labels_batch, tf.float32))\n",
    "  tf.add_to_collection(\"summary_op\", tf.summary.merge_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(video_id_batch, prediction_batch, label_batch, loss,\n",
    "                    summary_op, saver, summary_writer, evl_metrics,\n",
    "                    last_global_step_val):\n",
    "  \"\"\"Run the evaluation loop once.\n",
    "\n",
    "  Args:\n",
    "    video_id_batch: a tensor of video ids mini-batch.\n",
    "    prediction_batch: a tensor of predictions mini-batch.\n",
    "    label_batch: a tensor of label_batch mini-batch.\n",
    "    loss: a tensor of loss for the examples in the mini-batch.\n",
    "    summary_op: a tensor which runs the tensorboard summary operations.\n",
    "    saver: a tensorflow saver to restore the model.\n",
    "    summary_writer: a tensorflow summary_writer\n",
    "    evl_metrics: an EvaluationMetrics object.\n",
    "    last_global_step_val: the global step used in the previous evaluation.\n",
    "\n",
    "  Returns:\n",
    "    The global_step used in the latest model.\n",
    "  \"\"\"\n",
    "\n",
    "  global_step_val = -1\n",
    "  with tf.Session() as sess:\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.train_dir)\n",
    "    if latest_checkpoint:\n",
    "      logging.info(\"Loading checkpoint for eval: \" + latest_checkpoint)\n",
    "      # Restores from checkpoint\n",
    "      saver.restore(sess, latest_checkpoint)\n",
    "      # Assuming model_checkpoint_path looks something like:\n",
    "      # /my-favorite-path/yt8m_train/model.ckpt-0, extract global_step from it.\n",
    "      global_step_val = latest_checkpoint.split(\"/\")[-1].split(\"-\")[-1]\n",
    "    else:\n",
    "      logging.info(\"No checkpoint file found.\")\n",
    "      return global_step_val\n",
    "\n",
    "    if global_step_val == last_global_step_val:\n",
    "      logging.info(\"skip this checkpoint global_step_val=%s \"\n",
    "                   \"(same as the previous one).\", global_step_val)\n",
    "      return global_step_val\n",
    "\n",
    "    sess.run([tf.local_variables_initializer()])\n",
    "\n",
    "    # Start the queue runners.\n",
    "    fetches = [video_id_batch, prediction_batch, label_batch, loss, summary_op]\n",
    "    coord = tf.train.Coordinator()\n",
    "    try:\n",
    "      threads = []\n",
    "      for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
    "        threads.extend(qr.create_threads(\n",
    "            sess, coord=coord, daemon=True,\n",
    "            start=True))\n",
    "      logging.info(\"enter eval_once loop global_step_val = %s. \",\n",
    "                   global_step_val)\n",
    "\n",
    "      evl_metrics.clear()\n",
    "\n",
    "      examples_processed = 0\n",
    "      while not coord.should_stop():\n",
    "        batch_start_time = time.time()\n",
    "        _, predictions_val, labels_val, loss_val, summary_val = sess.run(\n",
    "            fetches)\n",
    "\n",
    "        seconds_per_batch = time.time() - batch_start_time\n",
    "        example_per_second = labels_val.shape[0] / seconds_per_batch\n",
    "        examples_processed += labels_val.shape[0]\n",
    "\n",
    "        iteration_info_dict = evl_metrics.accumulate(predictions_val,\n",
    "                                                     labels_val, loss_val, gt_labels)\n",
    "        iteration_info_dict[\"examples_per_second\"] = example_per_second\n",
    "\n",
    "        iterinfo = utils.AddGlobalStepSummary(\n",
    "            summary_writer,\n",
    "            global_step_val,\n",
    "            iteration_info_dict,\n",
    "            summary_scope=\"Eval\")\n",
    "        logging.info(\"examples_processed: %d | %s\", examples_processed,\n",
    "                     iterinfo)\n",
    "\n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "      logging.info(\n",
    "          \"Done with batched inference. Now calculating global performance \"\n",
    "          \"metrics.\")\n",
    "      # calculate the metrics for the entire epoch\n",
    "      epoch_info_dict = evl_metrics.get()\n",
    "      epoch_info_dict[\"epoch_id\"] = global_step_val\n",
    "\n",
    "      summary_writer.add_summary(summary_val, global_step_val)\n",
    "      epochinfo = utils.AddEpochSummary(\n",
    "          summary_writer,\n",
    "          global_step_val,\n",
    "          epoch_info_dict,\n",
    "          gt_labels,\n",
    "          summary_scope=\"Eval\")\n",
    "      logging.info(epochinfo)\n",
    "      evl_metrics.clear()\n",
    "    except Exception as e:  # pylint: disable=broad-except\n",
    "      logging.info(\"Unexpected exception: \" + str(e))\n",
    "      coord.request_stop(e)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads, stop_grace_period_secs=10)\n",
    "\n",
    "    return global_step_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using batch size of 512 for evaluation.\n",
      "INFO:tensorflow:number of evaluation files: 617\n",
      "INFO:tensorflow:built evaluation graph\n",
      "INFO:tensorflow:Loading checkpoint for eval: models\\model.ckpt-23590\n",
      "INFO:tensorflow:Restoring parameters from models\\model.ckpt-23590\n",
      "INFO:tensorflow:enter eval_once loop global_step_val = 23590. \n",
      "INFO:tensorflow:examples_processed: 512 | global_step 23590 | Batch Hit@1: 0.775 | Batch PERR: 0.669 | Batch Loss: 5.330 | Examples_per_sec: 153.518\n",
      "INFO:tensorflow:examples_processed: 935 | global_step 23590 | Batch Hit@1: 0.797 | Batch PERR: 0.689 | Batch Loss: 4.362 | Examples_per_sec: 447.291\n",
      "INFO:tensorflow:Done with batched inference. Now calculating global performance metrics.\n",
      "INFO:tensorflow:epoch/eval number 23590 | Avg_Hit@1: 0.785 | Avg_PERR: 0.678 | MAP: 0.486 | GAP: 0.648 | Avg_Loss: 4.892258\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(0)  # for reproducibility\n",
    "with tf.Graph().as_default():\n",
    "    # convert feature_names and feature_sizes to lists of values\n",
    "    feature_names, feature_sizes = utils.GetListOfFeatureNamesAndSizes(\n",
    "        FLAGS.feature_names, FLAGS.feature_sizes)\n",
    "    num_classes = FLAGS.num_classes\n",
    "\n",
    "    if FLAGS.frame_features:\n",
    "      reader = readers.YT8MFrameFeatureReader(\n",
    "          num_classes=num_classes,\n",
    "          feature_names=feature_names,feature_sizes=feature_sizes)\n",
    "    else:\n",
    "      reader = readers.YT8MAggregatedFeatureReader(\n",
    "          num_classes=num_classes,\n",
    "          feature_names=feature_names, feature_sizes=feature_sizes)\n",
    "\n",
    "    model = find_class_by_name(FLAGS.model,\n",
    "        [frame_level_models, video_level_models])()\n",
    "    label_loss_fn = find_class_by_name(FLAGS.label_loss, [losses])()\n",
    "\n",
    "    if FLAGS.eval_data_pattern is \"\":\n",
    "        raise IOError(\"'eval_data_pattern' was not specified. \" +\n",
    "                     \"Nothing to evaluate.\")\n",
    "\n",
    "    build_graph(\n",
    "        reader=reader,\n",
    "        model=model,\n",
    "        eval_data_pattern=FLAGS.eval_data_pattern,\n",
    "        label_loss_fn=label_loss_fn,\n",
    "        num_readers=FLAGS.num_readers,\n",
    "        batch_size=FLAGS.batch_size)\n",
    "    logging.info(\"built evaluation graph\")\n",
    "    video_id_batch = tf.get_collection(\"video_id_batch\")[0]\n",
    "    prediction_batch = tf.get_collection(\"predictions\")[0]\n",
    "    label_batch = tf.get_collection(\"labels\")[0]\n",
    "    loss = tf.get_collection(\"loss\")[0]\n",
    "    summary_op = tf.get_collection(\"summary_op\")[0]\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    summary_writer = tf.summary.FileWriter(\n",
    "        FLAGS.train_dir, graph=tf.get_default_graph())\n",
    "\n",
    "    evl_metrics = eval_util.EvaluationMetrics(reader.num_classes, FLAGS.top_k)\n",
    "\n",
    "    last_global_step_val = -1\n",
    "    while True:\n",
    "      last_global_step_val = evaluation_loop(video_id_batch, prediction_batch,\n",
    "                                             label_batch, loss, summary_op,\n",
    "                                             saver, summary_writer, evl_metrics,\n",
    "                                             last_global_step_val)\n",
    "      if FLAGS.run_once:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot evaluation results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>aps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Gunshot SC9mm</td>\n",
       "      <td>0.929844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Gunshot, gunfire</td>\n",
       "      <td>0.927400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vehicle</td>\n",
       "      <td>0.853547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Animal</td>\n",
       "      <td>0.841745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rail transport</td>\n",
       "      <td>0.827965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Domestic animals, pets</td>\n",
       "      <td>0.772331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Emergency vehicle</td>\n",
       "      <td>0.767126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Motor vehicle (road)</td>\n",
       "      <td>0.732695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Speech</td>\n",
       "      <td>0.666006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Tools</td>\n",
       "      <td>0.628259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Engine</td>\n",
       "      <td>0.618328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chainsaw</td>\n",
       "      <td>0.611416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Train wheels squealing</td>\n",
       "      <td>0.607289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Music</td>\n",
       "      <td>0.597913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conversation</td>\n",
       "      <td>0.596555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Machine gun</td>\n",
       "      <td>0.557905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Police car (siren)</td>\n",
       "      <td>0.556850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Fire engine, fire truck (siren)</td>\n",
       "      <td>0.548485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Aircraft</td>\n",
       "      <td>0.545881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Car</td>\n",
       "      <td>0.500231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Gunshot SC556</td>\n",
       "      <td>0.448985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hammer</td>\n",
       "      <td>0.405274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Idling</td>\n",
       "      <td>0.386630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Musical instrument</td>\n",
       "      <td>0.365736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Power tool</td>\n",
       "      <td>0.359127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ambulance (siren)</td>\n",
       "      <td>0.357173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Accelerating, revving, vroom</td>\n",
       "      <td>0.351861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Motorcycle</td>\n",
       "      <td>0.342936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Boat, Water vehicle</td>\n",
       "      <td>0.315813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Jackhammer</td>\n",
       "      <td>0.190606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Medium engine (mid frequency)</td>\n",
       "      <td>0.159127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Car passing by</td>\n",
       "      <td>0.150090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Vehicle horn, car horn, honking</td>\n",
       "      <td>0.146483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Traffic noise, roadway noise</td>\n",
       "      <td>0.114524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bus</td>\n",
       "      <td>0.107592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              label       aps\n",
       "35                    Gunshot SC9mm  0.929844\n",
       "15                 Gunshot, gunfire  0.927400\n",
       "6                           Vehicle  0.853547\n",
       "2                            Animal  0.841745\n",
       "9                    Rail transport  0.827965\n",
       "3            Domestic animals, pets  0.772331\n",
       "21                Emergency vehicle  0.767126\n",
       "8              Motor vehicle (road)  0.732695\n",
       "0                            Speech  0.666006\n",
       "11                            Tools  0.628259\n",
       "27                           Engine  0.618328\n",
       "28                         Chainsaw  0.611416\n",
       "34           Train wheels squealing  0.607289\n",
       "4                             Music  0.597913\n",
       "1                      Conversation  0.596555\n",
       "16                      Machine gun  0.557905\n",
       "22               Police car (siren)  0.556850\n",
       "24  Fire engine, fire truck (siren)  0.548485\n",
       "10                         Aircraft  0.545881\n",
       "17                              Car  0.500231\n",
       "36                    Gunshot SC556  0.448985\n",
       "12                           Hammer  0.405274\n",
       "32                           Idling  0.386630\n",
       "5                Musical instrument  0.365736\n",
       "14                       Power tool  0.359127\n",
       "23                Ambulance (siren)  0.357173\n",
       "33     Accelerating, revving, vroom  0.351861\n",
       "25                       Motorcycle  0.342936\n",
       "7               Boat, Water vehicle  0.315813\n",
       "13                       Jackhammer  0.190606\n",
       "29    Medium engine (mid frequency)  0.159127\n",
       "19                   Car passing by  0.150090\n",
       "18  Vehicle horn, car horn, honking  0.146483\n",
       "26     Traffic noise, roadway noise  0.114524\n",
       "20                              Bus  0.107592"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('csv_files/class_labels_indices_score.csv', sep=\";\")\n",
    "df.sort_values(by=['aps'], ascending=False)\n",
    "\n",
    "dff = df.query('aps > 0.1')\n",
    "dff = dff.sort_values(by=['aps'], ascending=False)\n",
    "\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(correct_labels, predict_labels, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
