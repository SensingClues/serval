{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code die checkt of wij wav-files correct wegschrijven naar tf-records.\n",
    "\n",
    "\n",
    "[link to original code](https://github.com/tensorflow/models/tree/master/research/audioset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "## Import packages\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging\n",
    "\n",
    "import vggish_input\n",
    "import vggish_postprocess\n",
    "import vggish_params\n",
    "import vggish_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'audio_embedding_feature_name', 'audio_embedding',\n",
    "    'Path to the VGGish checkpoint file.')\n",
    "\n",
    "## De volgende bestanden komen uit het eerdere mapje 'models'. Nu heb ik ze in yt8m gezet.\n",
    "## Deze zijn nodig om de embedding layer te maken\n",
    "flags.DEFINE_string(\n",
    "    'pca_params', 'models/vggish_pca_params.npz',\n",
    "    'Path to the VGGish PCA parameters file.')\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'checkpoint', 'models/vggish_model.ckpt',\n",
    "    'Path to the VGGish checkpoint file.')\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'yt_checkpoint', 'models/youtube_model.ckpt',\n",
    "    'Path to the VGGish checkpoint file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -2 voor silence\n",
    "tfrecord_name = \"test_embedding/-2.tfrecord\"\n",
    "wav_name = \"test_embedding/youtube-2.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_video_id = [b'-2NVPmjojzE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Haal hier de example uit de tf-record (zoekend op video-id en dan handmatig checken op correcte seconden)\n",
    "\n",
    "record_iterator = tf.python_io.tf_record_iterator(tfrecord_name)\n",
    "\n",
    "## Met break: check alleen het eerste geluidsfragment uit de tf-record\n",
    "embedding = None\n",
    "    \n",
    "for string_record in record_iterator:\n",
    "\n",
    "    ## Take one example\n",
    "    example = tf.train.SequenceExample()\n",
    "    example.ParseFromString(string_record)\n",
    "\n",
    "    video_id = example.context.feature['video_id'].bytes_list.value\n",
    "    \n",
    "    #video_id = str(video_id)[3:-2]\n",
    "    if(video_id == target_video_id):\n",
    "        embedding = example.feature_lists.feature_list['audio_embedding']\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding.feature[0].bytes_list.value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 46 81 138 66 105 0 144 155 156 131 105 146 238 106 78 0 235 145 153 0 54 100 227 225 136 80 60 174 152 72 255 189 141 255 112 158 141 0 86 233 0 255 53 173 73 208 178 108 66 161 239 118 201 132 174 229 121 130 15 137 124 255 92 0 57 177 210 123 0 255 137 99 130 135 140 47 152 255 0 0 167 242 255 121 0 109 242 245 0 16 255 0 99 149 0 234 0 0 0 211 255 158 239 87 149 110 153 255 85 255 255 215 86 69 216 82 246 46 0 165 0 108 188 86 80 68 0 "
     ]
    }
   ],
   "source": [
    "# Eerste seconde\n",
    "for byte in embedding.feature[1].bytes_list.value[0]:\n",
    "    print(byte, end='')\n",
    "    print(\" \", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Deze code is van inference.py gehaald.\n",
    "## Op het einde wordt er een nieuwe sequence example gemaakt\n",
    "\n",
    "## Function that takes examples from wav-file as input and returns a sequence example\n",
    "def getSequenceExample(examples_batch, labels):\n",
    "    \n",
    "    # Prepare a postprocessor to munge the model embeddings.\n",
    "    pproc = vggish_postprocess.Postprocessor(FLAGS.pca_params)\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "        # Define the model: load the checkpoint and locate input and output tensors\n",
    "        # Input: [batch_size, num_frames, num_bands] \n",
    "        # where [num_frames, num_bands] represents log-mel-scale spectrogram\n",
    "        # Output: embeddings\n",
    "        vggish_slim.define_vggish_slim(training=False)\n",
    "        vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n",
    "\n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.VGGISH_INPUT_TENSOR_NAME)\n",
    "        embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.VGGISH_OUTPUT_TENSOR_NAME)\n",
    "\n",
    "        # Run inference and postprocessing.\n",
    "        [embedding_batch] = sess.run([embedding_tensor],\n",
    "                                     feed_dict={features_tensor: examples_batch})\n",
    "        print(embedding_batch)\n",
    "\n",
    "        postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "        print(postprocessed_batch)\n",
    "\n",
    "\n",
    "        ## Maak sequence example\n",
    "        seq_example = tf.train.SequenceExample(\n",
    "            feature_lists = tf.train.FeatureLists(\n",
    "                feature_list={\n",
    "                    FLAGS.audio_embedding_feature_name:\n",
    "                        tf.train.FeatureList(\n",
    "                            feature=[\n",
    "                                tf.train.Feature(\n",
    "                                    bytes_list=tf.train.BytesList(\n",
    "                                        value=[embedding.tobytes()]))\n",
    "                                for embedding in postprocessed_batch\n",
    "                            ]\n",
    "                        )\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    return(embedding_batch, postprocessed_batch, seq_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples shape: (10, 96, 64)\n",
      "INFO:tensorflow:Restoring parameters from models/vggish_model.ckpt\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.22877741 0.17769393]\n",
      " [0.         0.         0.         ... 0.         0.22877741 0.17769393]]\n",
      "[[165   0 143 ...   0  22 255]\n",
      " [165   0 142 ...   0  51 255]\n",
      " [165   0 142 ...   0  51 255]\n",
      " ...\n",
      " [166   0 147 ...   0   0 255]\n",
      " [175   8 146 ... 147  69 255]\n",
      " [175   8 146 ... 147  69 255]]\n"
     ]
    }
   ],
   "source": [
    "## Maak eerst de examples\n",
    "examples_batch = vggish_input.wavfile_to_examples(wav_name)\n",
    "print(\"Examples shape: \" + str(examples_batch.shape))\n",
    "\n",
    "## Maak nu de embedding (met functie hierboven), pas PCA toe en maak er bytes van\n",
    "embedding_batch, postprocessed_batch, seq_example = getSequenceExample(examples_batch, [161])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.22877741,\n",
       "        0.17769393],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.22877741,\n",
       "        0.17769393]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([165,   0, 143, 116, 157,  61,  37,  95, 164, 203, 132,   0, 141,\n",
       "       232, 117,  83,  15, 255, 194,  58,   6,  50, 126, 162, 255,  64,\n",
       "       163, 130,  60, 193, 203, 255, 142,  74, 117, 189, 139, 255,  21,\n",
       "        56, 115,   0, 207,  67, 109, 191, 241, 158,   9, 165, 175, 184,\n",
       "       179, 145, 255, 204, 255,  57, 134, 164, 136, 213, 255,   7, 113,\n",
       "       121,  93, 179, 172,  62, 201,   6, 215, 134, 119,  80,  77, 242,\n",
       "       255,   0,   0, 158, 248, 255, 255, 208, 158, 255, 255,  97,   0,\n",
       "       222,  87, 136, 255,  10, 237,   0,  78,   0,  95, 255, 245, 255,\n",
       "         0, 102, 174,   0, 255, 202, 255, 255,   0,   7, 255, 255, 255,\n",
       "       255,   0, 255,  51,   0,   0, 164, 237,   0,  22, 255], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocessed_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xa5\\x00\\x8ft\\x9d=%_\\xa4\\xcb\\x84\\x00\\x8d\\xe8uS\\x0f\\xff\\xc2:\\x062~\\xa2\\xff@\\xa3\\x82<\\xc1\\xcb\\xff\\x8eJu\\xbd\\x8b\\xff\\x158s\\x00\\xcfCm\\xbf\\xf1\\x9e\\t\\xa5\\xaf\\xb8\\xb3\\x91\\xff\\xcc\\xff9\\x86\\xa4\\x88\\xd5\\xff\\x07qy]\\xb3\\xac>\\xc9\\x06\\xd7\\x86wPM\\xf2\\xff\\x00\\x00\\x9e\\xf8\\xff\\xff\\xd0\\x9e\\xff\\xffa\\x00\\xdeW\\x88\\xff\\n\\xed\\x00N\\x00_\\xff\\xf5\\xff\\x00f\\xae\\x00\\xff\\xca\\xff\\xff\\x00\\x07\\xff\\xff\\xff\\xff\\x00\\xff3\\x00\\x00\\xa4\\xed\\x00\\x16\\xff'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte_string_2 = seq_example.feature_lists.feature_list['audio_embedding'].feature[0].bytes_list.value[0]\n",
    "byte_string_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[165   0 143 116 157  61  37  95 164 203 132   0 141 232 117  83  15 255\n",
      " 194  58   6  50 126 162 255  64 163 130  60 193 203 255 142  74 117 189\n",
      " 139 255  21  56 115   0 207  67 109 191 241 158   9 165 175 184 179 145\n",
      " 255 204 255  57 134 164 136 213 255   7 113 121  93 179 172  62 201   6\n",
      " 215 134 119  80  77 242 255   0   0 158 248 255 255 208 158 255 255  97\n",
      "   0 222  87 136 255  10 237   0  78   0  95 255 245 255   0 102 174   0\n",
      " 255 202 255 255   0   7 255 255 255 255   0 255  51   0   0 164 237   0\n",
      "  22 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "bs = np.fromstring(byte_string_2, dtype=np.uint8)\n",
    "print(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([115,  46,  90, 120,  40, 117,   0, 180, 161, 157, 132,  38, 165,\n",
       "       246,  77, 106,   0, 255, 160, 133,   0,  25, 155, 242, 248,  72,\n",
       "        72,  53, 191, 228,  83, 255, 132, 101, 248, 144,  86, 152,   0,\n",
       "        61, 255,   0, 255,  40, 170,  87, 251, 224, 114,  63, 146, 235,\n",
       "       154, 184, 159, 220, 255,  64, 133,   0, 140, 175, 255,  61,  20,\n",
       "        94, 118, 202, 134,  22, 255, 121,  98, 111, 149, 103,  24, 237,\n",
       "       255,   0,   0, 151, 193, 255, 139,  25, 142, 170, 200,  36,   0,\n",
       "       255,   0, 110, 236,  50, 240,   0,   0,   0, 166, 255, 139, 255,\n",
       "       110,  46, 139, 150, 255,   0, 255, 255, 194,   0, 161, 233, 238,\n",
       "       211, 133,   0, 113,   0, 137, 255, 109,   1, 103,   0], dtype=uint8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Terug naar tf-record om te vergelijken\n",
    "bs_2 = np.fromstring(embedding.feature[0].bytes_list.value[0], dtype=np.uint8)\n",
    "bs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
