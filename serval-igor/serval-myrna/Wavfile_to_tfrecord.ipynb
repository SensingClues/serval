{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "## Import packages\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import gfile\n",
    "\n",
    "import vggish_input\n",
    "import vggish_postprocess\n",
    "import vggish_params\n",
    "import vggish_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'audio_embedding_feature_name', 'audio_embedding',\n",
    "    'Path to the VGGish checkpoint file.')\n",
    "\n",
    "## Map met csv-files\n",
    "flags.DEFINE_string(\n",
    "    'csvfile_path', 'csv_files/',\n",
    "    'Path to a csv files.')\n",
    "\n",
    "## Vul hier de map in waarin alle wav-files staan\n",
    "flags.DEFINE_string(\n",
    "    'wavfile_path', 'wav_files/',\n",
    "    'Path to wav files')\n",
    "\n",
    "## Map waar de nieuwe tf-records terecht komen\n",
    "flags.DEFINE_string(\n",
    "    'tfrecord_path', 'tfrecords/added_data/',\n",
    "    'Path to a TFRecord file where embeddings will be written.')\n",
    "\n",
    "## De volgende bestanden komen uit het eerdere mapje 'models'. Nu heb ik ze in yt8m gezet.\n",
    "flags.DEFINE_string(\n",
    "    'pca_params', 'yt8m/vggish_pca_params.npz',\n",
    "    'Path to the VGGish PCA parameters file.')\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'checkpoint', 'yt8m/vggish_model.ckpt',\n",
    "    'Path to the VGGish checkpoint file.')\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'yt_checkpoint', 'yt8m/youtube_model.ckpt',\n",
    "    'Path to the VGGish checkpoint file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Voor alle wav-files in een map, gebeurt het volgende:\n",
    "\n",
    "## Eerst wordt het wav-file ingelezen en de bijbehorende label(s) opgezocht in een csv-bestand\n",
    "## Als video-id wordt een willekeurig id'tje gegeven (in dit geval allemaal dezelfde)\n",
    "\n",
    "## Dan parsen we wav-file naar embeddings: \n",
    "# (dit gebeurt door vggish_input.wavfile_to_example aan te roepen)\n",
    "# Stap 1a: lezen van wav-file, input is array met samples die db aanduiden. Ook sample rate (per sec) wordt gelezen\n",
    "# Stap 1b: Bij 2d array (stereo, ipv mono) bereken gemiddelde, daarna normaliseren (delen door 32.768)\n",
    "# Stap 2: Bepaal examples in vorm [batch size, num frames, num bands].\n",
    "    # Hierbij worden voor verschillende batches (omdat alles tegelijk niet in 1x in NN kan),\n",
    "    # een log mel spectrogram gemaakt (in vorm [num_frames, num_bands])\n",
    "# Stap 3: Bepaal features: nu wordt de embedding laag gemaakt (PCA-components, discreet maken etc)\n",
    "    # Hiervoor worden model-parameters opgehaald die eerder zijn opgeslagen\n",
    "    \n",
    "## Daarna wordt een sequence example gemaakt (in getSequenceExample) en het als tf-records weggeschreven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Function that takes examples from wav-file as input and returns a sequence example\n",
    "\n",
    "def getSequenceExample(examples_batch, labels, video_id=[b'-1LrH01Ei1w']):\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "        # Define the model: load the checkpoint and locate input and output tensors\n",
    "        # Input: [batch_size, num_frames, num_bands] \n",
    "        # where [num_frames, num_bands] represents log-mel-scale spectrogram\n",
    "        # Output: embeddings\n",
    "        vggish_slim.define_vggish_slim(training=False)\n",
    "        vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n",
    "\n",
    "        pca_params = np.load(FLAGS.pca_params)\n",
    "        pca_matrix = pca_params[vggish_params.PCA_EIGEN_VECTORS_NAME]\n",
    "        pca_means = pca_params[vggish_params.PCA_MEANS_NAME].reshape(-1, 1)\n",
    "\n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.VGGISH_INPUT_TENSOR_NAME)\n",
    "        embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "            vggish_params.VGGISH_OUTPUT_TENSOR_NAME)\n",
    "        vggish_slim.load_youtube_model(sess, FLAGS.yt_checkpoint)\n",
    "\n",
    "        # Run inference and postprocessing\n",
    "        [embedding_batch] = sess.run([embedding_tensor],\n",
    "                                     feed_dict={features_tensor: examples_batch})\n",
    "\n",
    "        postprocessed_batch = np.dot(\n",
    "                pca_matrix, (embedding_batch.T - pca_means)\n",
    "            ).T\n",
    "        #print(postprocessed_batch)\n",
    "\n",
    "        num_frames = np.minimum(postprocessed_batch.shape[0], vggish_params.MAX_FRAMES)\n",
    "        data = vggish_postprocess.resize(postprocessed_batch, 0, vggish_params.MAX_FRAMES)\n",
    "        data = np.expand_dims(data, 0)\n",
    "        num_frames = np.expand_dims(num_frames, 0)\n",
    "\n",
    "        input_tensor = sess.graph.get_collection(\"input_batch_raw\")[0]\n",
    "        num_frames_tensor = sess.graph.get_collection(\"num_frames\")[0]\n",
    "\n",
    "        label_feat = tf.train.Feature(int64_list=tf.train.Int64List(value=labels))\n",
    "        videoid_feat = tf.train.Feature(bytes_list=tf.train.BytesList(value=video_id))\n",
    "\n",
    "        seq_example = tf.train.SequenceExample(\n",
    "            context = tf.train.Features(feature={\"labels\": label_feat, \"video_id\": videoid_feat}),\n",
    "            feature_lists = tf.train.FeatureLists(\n",
    "                feature_list={\n",
    "                    FLAGS.audio_embedding_feature_name:\n",
    "                        tf.train.FeatureList(\n",
    "                            feature=[\n",
    "                                tf.train.Feature(\n",
    "                                    bytes_list=tf.train.BytesList(\n",
    "                                        value=[embedding.tobytes()]))\n",
    "                                for embedding in postprocessed_batch\n",
    "                            ]\n",
    "                        )\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return(seq_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabels(file):\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: wav_files\\car_18.wav\n",
      "Examples shape: (11, 96, 64)\n",
      "INFO:tensorflow:Restoring parameters from yt8m/vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from yt8m/youtube_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "## Lees stuk voor stuk alle wav-files in\n",
    "## Zoek het bijbehorende label op in een csv-bestand\n",
    "## Bepaal de embeddings\n",
    "\n",
    "# Prepare a record writer to store the postprocessed embeddings.\n",
    "tfrecord_file = str(FLAGS.tfrecord_path + 'gun.tfrecord')\n",
    "writer = tf.python_io.TFRecordWriter(tfrecord_file)\n",
    "\n",
    "## Read and rewrite all test files\n",
    "files = gfile.Glob(str(FLAGS.wavfile_path + \"*.wav\"))\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    print(\"Filename: \" + str(file))\n",
    "   \n",
    "    ## Find labels belonging to wav-file\n",
    "    ## 15=Gunshots/gunfire, 35=Gunshot Sensing clues\n",
    "    labels = [15,35]\n",
    "    \n",
    "    ## This function reads the wav file and converts the samples into np arrays of [batch size, num frames, num bands]\n",
    "    #examples_batch = vggish_input.wavfile_to_examples(str(FLAGS.wavfile_path + wav_file))\n",
    "    examples_batch = vggish_input.wavfile_to_examples(file)\n",
    "    print(\"Examples shape: \" + str(examples_batch.shape))\n",
    "   \n",
    "    seq_example = getSequenceExample(examples_batch, labels)\n",
    "    \n",
    "    writer.write(seq_example.SerializeToString())\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
