{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "## Import packages\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "#from scipy.io import wavfile\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_string('output_tfrecords_file', 'tfrecords/added_train_data',\n",
    "                    'File containing tfrecords will be written at this path.')\n",
    "flags.DEFINE_string('model_dir', 'yt8m',\n",
    "                    'Directory to store model files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following flags are set to match the YouTube-8M dataset format.\n",
    "flags.DEFINE_integer('frames_per_second', 1,\n",
    "                   'This many frames per second will be processed')\n",
    "flags.DEFINE_string('labels_feature_key', 'labels',\n",
    "                  'Labels will be written to context feature with this '\n",
    "                  'key, as int64 list feature.')\n",
    "flags.DEFINE_string('image_feature_key', 'rgb',\n",
    "                  'Image features will be written to sequence feature with '\n",
    "                  'this key, as bytes list feature, with only one entry, '\n",
    "                  'containing quantized feature string.')\n",
    "flags.DEFINE_string('video_file_key_feature_key', 'video_id',\n",
    "                  'Input <video_file> will be written to context feature '\n",
    "                  'with this key, as bytes list feature, with only one '\n",
    "                  'entry, containing the file path of the video. This '\n",
    "                  'can be used for debugging but not for training or eval.')\n",
    "flags.DEFINE_boolean('insert_zero_audio_features', True,\n",
    "                   'If set, inserts features with name \"audio\" to be 128-D '\n",
    "                   'zero vectors. This allows you to use YouTube-8M '\n",
    "                   'pre-trained model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stap 1:\n",
    "## Download de volgende files en zet ze in model_dir (bv /yt8m)\n",
    "#'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "#'http://data.yt8m.org/yt8m_pca.tgz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load inception\n",
    "inception_proto_file = os.path.join(model_dir, 'classify_image_graph_def.pb')\n",
    "graph_def = tf.GraphDef.FromString(open(inception_proto_file, 'rb').read())\n",
    "inception_graph = tf.Graph()\n",
    "with inception_graph.as_default():\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n",
    "    session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pca\n",
    "pca_mean = np.load(\n",
    "    os.path.join(model_dir, 'mean.npy'))[:, 0]\n",
    "pca_eigenvals = np.load(\n",
    "    os.path.join(model_dir, 'eigenvals.npy'))[:1024, 0]\n",
    "pca_eigenvecs = np.load(\n",
    "    os.path.join(model_dir, 'eigenvecs.npy')).T[:, :1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.python_io.TFRecordWriter(FLAGS.output_tfrecords_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_written = 0\n",
    "total_error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_file, labels in csv.reader(open(FLAGS.input_videos_csv)):\n",
    "    rgb_features = []\n",
    "    for rgb in frame_iterator(video_file, every_ms=1000.0/FLAGS.frames_per_second):\n",
    "        features = extractor.extract_rgb_frame_features(rgb[:, :, ::-1])\n",
    "        rgb_features.append(_bytes_feature(quantize(features)))\n",
    "\n",
    "    if not rgb_features:\n",
    "        print >> sys.stderr, 'Could not get features for ' + video_file\n",
    "        total_error += 1\n",
    "        continue\n",
    "\n",
    "    # Create SequenceExample proto and write to output.\n",
    "    feature_list = {\n",
    "        FLAGS.image_feature_key: tf.train.FeatureList(feature=rgb_features),\n",
    "    }\n",
    "    if FLAGS.insert_zero_audio_features:\n",
    "      feature_list['audio'] = tf.train.FeatureList(\n",
    "          feature=[_bytes_feature(_make_bytes([0] * 128))] * len(rgb_features))\n",
    "\n",
    "    example = tf.train.SequenceExample(\n",
    "        context=tf.train.Features(feature={\n",
    "            FLAGS.labels_feature_key:\n",
    "                _int64_list_feature(sorted(map(int, labels.split(';')))),\n",
    "            FLAGS.video_file_key_feature_key:\n",
    "                _bytes_feature(_make_bytes(map(ord, video_file))),\n",
    "        }),\n",
    "        feature_lists=tf.train.FeatureLists(feature_list=feature_list))\n",
    "    writer.write(example.SerializeToString())\n",
    "    total_written += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In feature_extractor.py (init) zie je hoe je dat via Python-code kan doen\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myrna\\Anaconda3\\lib\\site-packages\\scipy\\io\\wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 96, 64)\n"
     ]
    }
   ],
   "source": [
    "## Stap 1 en 2\n",
    "## This function reads the wav file and converts the samples into np arrays of [batch size, num frames, num bands]\n",
    "examples_batch = vggish_input.wavfile_to_examples(FLAGS.wav_file)\n",
    "print(examples_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read csv-file with labels\n",
    "class_map = {}\n",
    "with open(vggish_params.CLASS_LABELS_INDICES) as f:\n",
    "    next(f)  # skip header\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        class_map[int(row[0])] = row[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/vggish_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/youtube_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "## Stap 3\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    # Define the model: load the checkpoint and locate input and output tensors\n",
    "    # Input: [batch_size, num_frames, num_bands] \n",
    "    # where [num_frames, num_bands] represents log-mel-scale spectrogram\n",
    "    # Output: embeddings\n",
    "    vggish_slim.define_vggish_slim(training=False)\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess, FLAGS.checkpoint)\n",
    "    \n",
    "    pca_params = np.load(vggish_params.VGGISH_PCA_PARAMS)\n",
    "    pca_matrix = pca_params[vggish_params.PCA_EIGEN_VECTORS_NAME]\n",
    "    pca_means = pca_params[vggish_params.PCA_MEANS_NAME].reshape(-1, 1)\n",
    "    \n",
    "    features_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.VGGISH_INPUT_TENSOR_NAME)\n",
    "    embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.VGGISH_OUTPUT_TENSOR_NAME)\n",
    "    vggish_slim.load_youtube_model(sess, vggish_params.YOUTUBE_CHECKPOINT_FILE)\n",
    "    \n",
    "    # Run inference and postprocessing\n",
    "    [embedding_batch] = sess.run([embedding_tensor],\n",
    "                                 feed_dict={features_tensor: examples_batch})\n",
    "    \n",
    "    postprocessed_batch = np.dot(\n",
    "            pca_matrix, (embedding_batch.T - pca_means)\n",
    "        ).T\n",
    "    #print(postprocessed_batch)\n",
    "    \n",
    "    num_frames = np.minimum(postprocessed_batch.shape[0], vggish_params.MAX_FRAMES)\n",
    "    data = vggish_postprocess.resize(postprocessed_batch, 0, vggish_params.MAX_FRAMES)\n",
    "    data = np.expand_dims(data, 0)\n",
    "    num_frames = np.expand_dims(num_frames, 0)\n",
    "    \n",
    "    input_tensor = sess.graph.get_collection(\"input_batch_raw\")[0]\n",
    "    num_frames_tensor = sess.graph.get_collection(\"num_frames\")[0]\n",
    "    predictions_tensor = sess.graph.get_collection(\"predictions\")[0]\n",
    "    \n",
    "    ## Stap 4\n",
    "    predictions_val, = sess.run(\n",
    "        [predictions_tensor],\n",
    "        feed_dict={\n",
    "            input_tensor: data,\n",
    "            num_frames_tensor: num_frames\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Filter predictions (give top 20 where p>0.1)\n",
    "count = vggish_params.PREDICTIONS_COUNT_LIMIT\n",
    "hit = vggish_params.PREDICTIONS_HIT_LIMIT\n",
    "top_indices = np.argpartition(predictions_val[0], -count)[-count:]\n",
    "line = ((class_map[i], float(predictions_val[0][i])) for i in top_indices if predictions_val[0][i] > hit)\n",
    "predictions = sorted(line, key=lambda p: -p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Vehicle', 0.9999919533729553), ('Music', 0.7836809158325195), ('Car', 0.7723241448402405), ('Bus', 0.7148009538650513), ('Toot', 0.4626094698905945)]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
